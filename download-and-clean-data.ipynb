{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "130b53f7-2a49-48dc-84c3-8a9dc7355fad",
   "metadata": {},
   "source": [
    "# Notebook to fetch the dataset and prepare it for use\n",
    "- Dataset location [Data for PAN at SemEval 2019 Task 4: Hyperpartisan News Detection](https://zenodo.org/records/1489920)\n",
    "- Also see [Hyperpartisan News Detection 2019](https://pan.webis.de/semeval19/semeval19-web/#data) and [SemEval-2019 Task 4: Hyperpartisan News Detection](https://aclanthology.org/S19-2145.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "616e76e2-ad2a-4455-8267-b8295b95bbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wget\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4259ab19-8d5b-4099-9abd-ca769247ef43",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetlinks = {\n",
    "    \"article.xsd\": \"https://zenodo.org/records/1489920/files/article.xsd?download=1\",\n",
    "    \"ground-truth.xsd\": \"https://zenodo.org/records/1489920/files/ground-truth.xsd?download=1\",\n",
    "    \"articles-training-byarticle-20181122.zip\": \"https://zenodo.org/records/1489920/files/articles-training-byarticle-20181122.zip?download=1\",\n",
    "    \"articles-validation-bypublisher-20181122.zip\": \"https://zenodo.org/records/1489920/files/articles-validation-bypublisher-20181122.zip?download=1\",\n",
    "    \"ground-truth-training-byarticle-20181122.zip\": \"https://zenodo.org/records/1489920/files/ground-truth-training-byarticle-20181122.zip?download=1\",\n",
    "    \"ground-truth-training-bypublisher-20181122.zip\": \"https://zenodo.org/records/1489920/files/ground-truth-training-bypublisher-20181122.zip?download=1\",\n",
    "    \"ground-truth-validation-bypublisher-20181122.zip\": \"https://zenodo.org/records/1489920/files/ground-truth-validation-bypublisher-20181122.zip?download=1\",\n",
    "}\n",
    "\n",
    "# Omitted from above, to save time, is the main training set with ~600k articles\n",
    "# I'm using the validation set (150k articles) with an 80/20 test/train split instead\n",
    "# To include that training set, add the line below:\n",
    "# \"articles-training-bypublisher-20181122.zip\": \"https://zenodo.org/records/1489920/files/articles-training-bypublisher-20181122.zip?download=1\","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03290cc9-89c7-4f7c-9c05-641413d9f3a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file /home/adhavle/hnd/dataset/article.xsd already exists\n",
      "file /home/adhavle/hnd/dataset/ground-truth.xsd already exists\n",
      "file /home/adhavle/hnd/dataset/articles-training-byarticle-20181122.zip already exists\n",
      "file /home/adhavle/hnd/dataset/articles-validation-bypublisher-20181122.zip already exists\n",
      "file /home/adhavle/hnd/dataset/ground-truth-training-byarticle-20181122.zip already exists\n",
      "file /home/adhavle/hnd/dataset/ground-truth-training-bypublisher-20181122.zip already exists\n",
      "file /home/adhavle/hnd/dataset/ground-truth-validation-bypublisher-20181122.zip already exists\n"
     ]
    }
   ],
   "source": [
    "dataset_dir = os.path.join(os.getcwd(), \"dataset\")\n",
    "os.makedirs(dataset_dir, exist_ok=True)\n",
    "\n",
    "for fname, url in datasetlinks.items():\n",
    "    filepath = os.path.join(dataset_dir, fname)\n",
    "    \n",
    "    if os.path.isfile(filepath):\n",
    "        print(f\"file {filepath} already exists\")\n",
    "    else:\n",
    "        print(f\"downloading {fname} from {url}\")\n",
    "        wget.download(url, os.path.join(dataset_dir, filepath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aecb65f0-aef6-4b31-a6de-4714ed486f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "unzip_targets = {\n",
    "    \"articles-training-byarticle-20181122.zip\": \"articles-training-byarticle-20181122.xml\",\n",
    "    \"articles-validation-bypublisher-20181122.zip\": \"articles-validation-bypublisher-20181122.xml\",\n",
    "    \"ground-truth-training-byarticle-20181122.zip\": \"ground-truth-training-byarticle-20181122.xml\",\n",
    "    \"ground-truth-training-bypublisher-20181122.zip\": \"ground-truth-training-bypublisher-20181122.xml\",\n",
    "    \"ground-truth-validation-bypublisher-20181122.zip\": \"ground-truth-validation-bypublisher-20181122.xml\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbe12b98-51a1-40f7-be67-21c4f7e2be26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file /home/adhavle/hnd/dataset/articles-training-byarticle-20181122.xml already unzipped\n",
      "file /home/adhavle/hnd/dataset/articles-validation-bypublisher-20181122.xml already unzipped\n",
      "file /home/adhavle/hnd/dataset/ground-truth-training-byarticle-20181122.xml already unzipped\n",
      "file /home/adhavle/hnd/dataset/ground-truth-training-bypublisher-20181122.xml already unzipped\n",
      "file /home/adhavle/hnd/dataset/ground-truth-validation-bypublisher-20181122.xml already unzipped\n"
     ]
    }
   ],
   "source": [
    "for zipname, target in unzip_targets.items():\n",
    "    file_target = os.path.join(dataset_dir, target)\n",
    "\n",
    "    if os.path.isfile(file_target):\n",
    "        print(f\"file {file_target} already unzipped\")\n",
    "    else:\n",
    "        zip_path = os.path.join(dataset_dir, zipname)\n",
    "        print(f\"unzipping {zip_path} to {file_target}\")\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(dataset_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dce942f-f39d-40cc-af0f-9dbdcb3671de",
   "metadata": {},
   "source": [
    "Since I'm planning to just use the validation set, I only actually need 2 of these files:\n",
    " - `articles-validation-bypublisher-20181122.xml` (150k articles - will use in an 80/20 test/train split)\n",
    " - `ground-truth-validation-bypublisher-20181122.xml` (the target/bias values for these articles, scored as `left`, `left-center`, `least`, `right-center`, `right`)\n",
    " - The first file contains un-escaped HTML tags within article contents. To avoid confusing Pandas' XML loader, I'll convert all angle braces within the articles to their HTML entity codes (so `<` becomes `&lt;` and `>` becomes `&gt;`)\n",
    " - I am only doing this cleaning step on `articles-validation-bypublisher-20181122.xml`, but anyone planning to use the other 2 'article' files will need to do the same on them as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6523f36-fa91-430a-93ed-b63307e96acd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned 150000 articles\n"
     ]
    }
   ],
   "source": [
    "infile = os.path.join(dataset_dir, \"articles-validation-bypublisher-20181122.xml\")\n",
    "outfile = os.path.join(dataset_dir, \"articles-validation-bypublisher-20181122-html-escaped.xml\")\n",
    "\n",
    "start_tag = u'<article'\n",
    "end_tag = u'</article>\\n'\n",
    "articles = 0\n",
    "\n",
    "with open(infile, \"r\", encoding='utf-8') as inf:\n",
    "    with open(outfile, \"w\", encoding='utf-8') as outf:\n",
    "\n",
    "        # read the XML header and opening articles tag\n",
    "        line = inf.readline()\n",
    "        if line.startswith(u\"<?xml version\"):\n",
    "            outf.write(line)\n",
    "        if u\"<articles>\" not in line:\n",
    "            line = inf.readline()\n",
    "            outf.write(line)\n",
    "\n",
    "        # clean the rest of the file\n",
    "        while True:\n",
    "            line = inf.readline()\n",
    "            if not line:\n",
    "                break\n",
    "\n",
    "            if line.startswith(start_tag):\n",
    "                articles += 1\n",
    "                close_article_tag = line.find(\"\\\">\")\n",
    "                article_tag = line[:close_article_tag + 2]\n",
    "\n",
    "                if line.endswith(end_tag):\n",
    "\n",
    "                    # handle single-line article\n",
    "                    end_article_tag = line.find(end_tag)\n",
    "                    content = line[close_article_tag + 2:end_article_tag]\n",
    "                    content = content.replace(\"<\", \"&lt;\").replace(\">\", \"&gt;\")\n",
    "                    outf.write(f\"{article_tag}{content}{end_tag}\")\n",
    "\n",
    "                else:\n",
    "\n",
    "                    # handle multi-line article\n",
    "                    content = line[close_article_tag + 2:]\n",
    "                    content = content.replace(\"<\", \"&lt;\").replace(\">\", \"&gt;\")\n",
    "                    outf.write(f\"{article_tag}{content}\")\n",
    "\n",
    "                    foundClosingTag = False\n",
    "                    while not foundClosingTag:\n",
    "                        line = inf.readline()\n",
    "                        if line.endswith(end_tag):\n",
    "                            foundClosingTag = True\n",
    "                            end_article_tag = line.find(end_tag)\n",
    "                            content = line[:end_article_tag]\n",
    "                            content = content.replace(\"<\", \"&lt;\").replace(\">\", \"&gt;\")\n",
    "                            outf.write(f\"{content}{end_tag}\")\n",
    "                        else:\n",
    "                            content = line.replace(\"<\", \"&lt;\").replace(\">\", \"&gt;\")\n",
    "                            outf.write(content)\n",
    "\n",
    "            else:\n",
    "                # not in an article - just replicate to output file\n",
    "                outf.write(line)\n",
    "\n",
    "print(f\"Cleaned {articles} articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fea373b-3bc4-46dc-845d-9c51ad285187",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
