{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8131c67d-087b-47d2-8e9d-efcdc0c02e21",
   "metadata": {},
   "source": [
    "Not ideal approach since the test/train split is being done after fit_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2542be24-5f35-41c8-ad9a-b6f2b45ffab9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-24T15:06:02.041381Z",
     "iopub.status.busy": "2025-08-24T15:06:02.040976Z",
     "iopub.status.idle": "2025-08-24T15:06:05.384049Z",
     "shell.execute_reply": "2025-08-24T15:06:05.383216Z",
     "shell.execute_reply.started": "2025-08-24T15:06:02.041364Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def dataset_file_path(fname):\n",
    "    return os.path.join(os.path.join(os.getcwd(), \"dataset\", fname))\n",
    "\n",
    "df = pd.read_pickle(dataset_file_path(\"articles_dataframe.pkl\"))\n",
    "\n",
    "additional_stopwords = ['amp', 'com', 'href', 'htm', 'html', 'http', 'https', 'php', 'searchindex', 'solr', 'www']\n",
    "stop_words = list(text.ENGLISH_STOP_WORDS.union(additional_stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bce3660d-b688-4b8a-b791-d3db3a024171",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T15:51:07.582737Z",
     "iopub.status.busy": "2025-08-23T15:51:07.582549Z",
     "iopub.status.idle": "2025-08-23T16:21:10.873301Z",
     "shell.execute_reply": "2025-08-23T16:21:10.872112Z",
     "shell.execute_reply.started": "2025-08-23T15:51:07.582722Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------\n",
      "max_features 1000, ngram_upper_range 1, min_df 5, max_df 0.6\n",
      "vec_time 24.4\n",
      "model_time 10.7\n",
      "model_score 0.721\n",
      "----------------\n",
      "max_features 1000, ngram_upper_range 1, min_df 5, max_df 0.7\n",
      "vec_time 24.0\n",
      "model_time 10.6\n",
      "model_score 0.721\n",
      "----------------\n",
      "max_features 1000, ngram_upper_range 1, min_df 10, max_df 0.6\n",
      "vec_time 24.5\n",
      "model_time 10.0\n",
      "model_score 0.721\n",
      "----------------\n",
      "max_features 1000, ngram_upper_range 1, min_df 10, max_df 0.7\n",
      "vec_time 24.5\n",
      "model_time 10.5\n",
      "model_score 0.721\n",
      "----------------\n",
      "max_features 1000, ngram_upper_range 2, min_df 5, max_df 0.6\n",
      "vec_time 124.9\n",
      "model_time 10.5\n",
      "model_score 0.723\n",
      "----------------\n",
      "max_features 1000, ngram_upper_range 2, min_df 5, max_df 0.7\n",
      "vec_time 125.5\n",
      "model_time 10.1\n",
      "model_score 0.723\n",
      "----------------\n",
      "max_features 1000, ngram_upper_range 2, min_df 10, max_df 0.6\n",
      "vec_time 126.3\n",
      "model_time 12.6\n",
      "model_score 0.723\n",
      "----------------\n",
      "max_features 1000, ngram_upper_range 2, min_df 10, max_df 0.7\n",
      "vec_time 127.3\n",
      "model_time 10.0\n",
      "model_score 0.723\n",
      "----------------\n",
      "max_features 2500, ngram_upper_range 1, min_df 5, max_df 0.6\n",
      "vec_time 23.8\n",
      "model_time 44.3\n",
      "model_score 0.772\n",
      "----------------\n",
      "max_features 2500, ngram_upper_range 1, min_df 5, max_df 0.7\n",
      "vec_time 25.1\n",
      "model_time 42.5\n",
      "model_score 0.772\n",
      "----------------\n",
      "max_features 2500, ngram_upper_range 1, min_df 10, max_df 0.6\n",
      "vec_time 24.9\n",
      "model_time 44.8\n",
      "model_score 0.772\n",
      "----------------\n",
      "max_features 2500, ngram_upper_range 1, min_df 10, max_df 0.7\n",
      "vec_time 24.0\n",
      "model_time 43.9\n",
      "model_score 0.772\n",
      "----------------\n",
      "max_features 2500, ngram_upper_range 2, min_df 5, max_df 0.6\n",
      "vec_time 126.3\n",
      "model_time 44.5\n",
      "model_score 0.779\n",
      "----------------\n",
      "max_features 2500, ngram_upper_range 2, min_df 5, max_df 0.7\n",
      "vec_time 125.8\n",
      "model_time 46.3\n",
      "model_score 0.779\n",
      "----------------\n",
      "max_features 2500, ngram_upper_range 2, min_df 10, max_df 0.6\n",
      "vec_time 126.1\n",
      "model_time 44.3\n",
      "model_score 0.778\n",
      "----------------\n",
      "max_features 2500, ngram_upper_range 2, min_df 10, max_df 0.7\n",
      "vec_time 127.0\n",
      "model_time 44.7\n",
      "model_score 0.778\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_score</th>\n",
       "      <th>max_features</th>\n",
       "      <th>ngram_upper_range</th>\n",
       "      <th>min_df</th>\n",
       "      <th>max_df</th>\n",
       "      <th>vectorizer_time</th>\n",
       "      <th>train_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.72</td>\n",
       "      <td>1000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.60</td>\n",
       "      <td>24.44</td>\n",
       "      <td>10.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.72</td>\n",
       "      <td>1000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.70</td>\n",
       "      <td>23.96</td>\n",
       "      <td>10.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.72</td>\n",
       "      <td>1000</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.60</td>\n",
       "      <td>24.46</td>\n",
       "      <td>10.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.72</td>\n",
       "      <td>1000</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.70</td>\n",
       "      <td>24.51</td>\n",
       "      <td>10.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.72</td>\n",
       "      <td>1000</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.60</td>\n",
       "      <td>124.87</td>\n",
       "      <td>10.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.72</td>\n",
       "      <td>1000</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.70</td>\n",
       "      <td>125.47</td>\n",
       "      <td>10.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.72</td>\n",
       "      <td>1000</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>0.60</td>\n",
       "      <td>126.26</td>\n",
       "      <td>12.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.72</td>\n",
       "      <td>1000</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>0.70</td>\n",
       "      <td>127.29</td>\n",
       "      <td>10.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.77</td>\n",
       "      <td>2500</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.60</td>\n",
       "      <td>23.79</td>\n",
       "      <td>44.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.77</td>\n",
       "      <td>2500</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.70</td>\n",
       "      <td>25.09</td>\n",
       "      <td>42.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.77</td>\n",
       "      <td>2500</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.60</td>\n",
       "      <td>24.87</td>\n",
       "      <td>44.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.77</td>\n",
       "      <td>2500</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.70</td>\n",
       "      <td>23.98</td>\n",
       "      <td>43.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.78</td>\n",
       "      <td>2500</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.60</td>\n",
       "      <td>126.32</td>\n",
       "      <td>44.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.78</td>\n",
       "      <td>2500</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.70</td>\n",
       "      <td>125.82</td>\n",
       "      <td>46.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.78</td>\n",
       "      <td>2500</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>0.60</td>\n",
       "      <td>126.14</td>\n",
       "      <td>44.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.78</td>\n",
       "      <td>2500</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>0.70</td>\n",
       "      <td>126.96</td>\n",
       "      <td>44.70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    model_score  max_features  ngram_upper_range  min_df  max_df  \\\n",
       "0          0.72          1000                  1       5    0.60   \n",
       "1          0.72          1000                  1       5    0.70   \n",
       "2          0.72          1000                  1      10    0.60   \n",
       "3          0.72          1000                  1      10    0.70   \n",
       "4          0.72          1000                  2       5    0.60   \n",
       "5          0.72          1000                  2       5    0.70   \n",
       "6          0.72          1000                  2      10    0.60   \n",
       "7          0.72          1000                  2      10    0.70   \n",
       "8          0.77          2500                  1       5    0.60   \n",
       "9          0.77          2500                  1       5    0.70   \n",
       "10         0.77          2500                  1      10    0.60   \n",
       "11         0.77          2500                  1      10    0.70   \n",
       "12         0.78          2500                  2       5    0.60   \n",
       "13         0.78          2500                  2       5    0.70   \n",
       "14         0.78          2500                  2      10    0.60   \n",
       "15         0.78          2500                  2      10    0.70   \n",
       "\n",
       "    vectorizer_time  train_time  \n",
       "0             24.44       10.73  \n",
       "1             23.96       10.64  \n",
       "2             24.46       10.03  \n",
       "3             24.51       10.47  \n",
       "4            124.87       10.49  \n",
       "5            125.47       10.14  \n",
       "6            126.26       12.60  \n",
       "7            127.29       10.03  \n",
       "8             23.79       44.29  \n",
       "9             25.09       42.46  \n",
       "10            24.87       44.81  \n",
       "11            23.98       43.95  \n",
       "12           126.32       44.52  \n",
       "13           125.82       46.28  \n",
       "14           126.14       44.28  \n",
       "15           126.96       44.70  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cv_max_features = [1000, 2500]\n",
    "cv_ngram_upper_range = [1, 2]\n",
    "cv_min_df = [5, 10]\n",
    "cv_max_df = [0.6, 0.7]\n",
    "\n",
    "# fetch cartesian product of feature ranges for grid search\n",
    "cv_params = list(product(cv_max_features, cv_ngram_upper_range, cv_min_df, cv_max_df))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df, \n",
    "                                                df['bias'], \n",
    "                                                test_size=0.2, \n",
    "                                                random_state=42, \n",
    "                                                stratify=df['bias'])\n",
    "\n",
    "\n",
    "cv_result = []\n",
    "for max_features, ngram_upper_range, min_df, max_df in cv_params:\n",
    "\n",
    "    print(\"----------------\")\n",
    "    print(f\"max_features {max_features}, ngram_upper_range {ngram_upper_range}, min_df {min_df}, max_df {max_df}\")\n",
    "\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        lowercase=True,\n",
    "        stop_words=stop_words, \n",
    "        max_features=max_features,\n",
    "        min_df=min_df,\n",
    "        max_df=max_df,\n",
    "        ngram_range=(1,ngram_upper_range))\n",
    "\n",
    "    vec_time = time.time()\n",
    "    bow_train = vectorizer.fit_transform(x_train['article'])\n",
    "    vec_time = time.time() - vec_time\n",
    "    print(f\"vec_time {vec_time:.1f}\")\n",
    "\n",
    "    bow_train_df = pd.DataFrame(\n",
    "        bow_train.toarray(), \n",
    "        columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "    logregmodel = LogisticRegression(max_iter=200, random_state=42, solver='lbfgs', penalty='l2')\n",
    "    model_time = time.time()\n",
    "    logregmodel.fit(bow_train_df, y_train)\n",
    "    model_time = time.time() - model_time\n",
    "\n",
    "    bow_test = vectorizer.transform(x_test['article'])\n",
    "    bow_test_df = pd.DataFrame(\n",
    "        bow_test.toarray(), \n",
    "        columns=vectorizer.get_feature_names_out())\n",
    "    \n",
    "    model_score = logregmodel.score(bow_test_df, y_test)\n",
    "\n",
    "    cv_result.append([model_score, max_features, ngram_upper_range, min_df, max_df, vec_time, model_time])\n",
    "    print(f\"model_time {model_time:.1f}\")\n",
    "    print(f\"model_score {model_score:.3f}\")\n",
    "\n",
    "cv_df = pd.DataFrame(\n",
    "    cv_result,\n",
    "    columns = ['model_score', 'max_features', 'ngram_upper_range', 'min_df', 'max_df', 'vectorizer_time', 'train_time'])\n",
    "pd.options.display.float_format = \"{:,.2f}\".format\n",
    "pd.options.display.max_rows = len(cv_result)\n",
    "display(cv_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423c42aa-1264-45a2-ab28-167000ae0d2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338a8cbf-d7e3-48a7-b15c-91861023c2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df, \n",
    "                                                df['bias'], \n",
    "                                                test_size=0.2, \n",
    "                                                random_state=42, \n",
    "                                                stratify=df['bias'])\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"vec\", TfidfVectorizer(lowercase = True, stop_words = stop_words)),\n",
    "    (\"lr\", LogisticRegression(max_iter=200, random_state=42, solver='lbfgs', penalty='l2')),\n",
    "])\n",
    "\n",
    "\n",
    "parameters = {\n",
    "    'vec__max_features': [1000, 2500],\n",
    "    'vec__ngram_range': [(1,1), (1,2)],\n",
    "    'vec__min_df': [5, 10],\n",
    "    'vec__max_df': [0.6, 0.7],\n",
    "}\n",
    "\n",
    "cv_grid = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid = parameters,\n",
    "    scoring= \"accuracy\",\n",
    "    verbose = 3,\n",
    "    n_jobs = 1)\n",
    "\n",
    "cv_grid.fit(x_train['article'], y_train)\n",
    "\n",
    "lrc = cv_grid.best_estimator_\n",
    "lrc.score(x_test['article'], y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
